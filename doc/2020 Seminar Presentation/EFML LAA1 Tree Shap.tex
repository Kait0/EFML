\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}

\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\DeclareMathOperator*{\argmin}{\arg\!\min}

    
\begin{document}

\title{Explaining Tree Models}

\author{
	\IEEEauthorblockN{Bernhard Jaeger}
	\IEEEauthorblockA{
		\textit{University of Tübingen}\\
		Tübingen, Germany \\
		bernhard.jaeger@student.uni-tuebingen.de
	}
}

\maketitle

\begin{abstract}
Understanding the decisions made by machine learning models can be challenging due to their black box nature.
We investigate the additive feature attribution method SHAP (SHapley Additive exPlanations) that addresses this problem \cite{b2}. 
While SHAP can be used to explain the decisions of any model, the generic KernelSHAP algorithm for it can be intractable for larger datasets. 
For tree models there exists a specialized algorithm (Tree SHAP) that can compute exact SHAP values in polynomial time \cite{b1}. 
We use the SHAP framework to investigate the relationship between Decision Trees and Hoeffding Trees\cite{b3}. 
Theory tells us that a Hoeffding Tree should converge towards a Decision Tree when trained. 
In our experiments however we could not observe convergence. 
The reason for this remains unclear.
\end{abstract}

\begin{IEEEkeywords}
Tree SHAP, SHAP, Decision Tree, Hoeffding Tree
\end{IEEEkeywords}

\section{Introduction}
This . This  and \cite{b3} document is a model and instructions for \LaTeX.
Please observe the conference page limits. 

\section{Additive Feature Attribution methods}
Modern models of Machine Learning have become increasingly complex making it hard to explain their decisions.
To address this challenge we can go one layer of abstraction further and make a model of the model which we want to be simple enough to be interpretable again.
In \cite{b2} they coined the term explanation model for this approach of using an interpretable approximation of the original model.\\
When viewed trough this lens multiple of the current explanation methods can be categorized in the class of  \textbf{Additive Feature Attribution methods}. In \cite{b2} they defined this as methods having an explanation model that is a linear function of binary variables:\\

\begin{align}
	g(z') = \phi_0 + \sum_{i=1}^{M}\phi_i z_{i}'
\end{align}

Where g is the explanation model, $z' \in \{0,1\}^M, \phi \in \mathbb{R}$. \\
\textit{Local methods} like this first approximate their model $f(x)$ using \textit{simplified inputs} $x'$ where $x = h_x(x')$.\\
The objective is then to ensure $g(z') \approx f(h_x(z'))$ whenever $z' \approx x'$. \\
$\phi_i$ is then called the \textbf{effect} of feature i and summing up all effects approximates the output of the model $f(x)$ we wish to explain. Note that these \textit{explanation models} are now easily interpretable. We can for a prediction $f(x)$ given input $x$  say what influence or effect each component $x_i$ had on the final decision.\\
Methods falling in the category of Additive Feature Attribution methods include LIME \cite{b4}, DeepLIFT\cite{b5}, layer-wise relevance propagation \cite{b6}, Quantitive Input Influence \cite{b7}, Shapley sampling values \cite{b8} and SHAP \cite{b2}.\\

Additive Feature Attribution methods can have 3 desirable theoretical properties \cite{b2}:\\

\textbf{Local Accuracy}: \\
\begin{align}
f(x) = g(x') when x = h_x(x') and \phi_0 = f(h_x(0))
\end{align}
The intuition being that the explanation model should match the original model for the simplified input (notice $x' = z'$ here). \\

\textbf{Missingness}: \\
\begin{align}
x'_i = 0 \rightarrow \phi_i = 0
\end{align}
The intuition is that feature missing in the original input may have no impact even if they are present in the simplified version. (Notice that even if $x' = 0$, $z'$ does not necessarily have to be 0).\\

\textbf{Consistency}: \\
$f_x(z') := f(h_x(z'))$, and let $z' \setminus i$ be $z'_i = 0$\\
For any two models f and f', if:
\begin{align}
f'_x(z') - f'_x(z' \setminus i) \geq f_x(z') - f_x(z' \setminus i)
\end{align}
for all inputs $z' \in {0,1}^M$. \\
Meaning that the model changes in a way that some simplified input's contribution becomes larger.\\
Then: $ \phi_i(f', x) \geq \phi_i(f,x)$ \\
Meaning that it's inputs attribution should not become smaller.\\

Now it turns out that the only possible features for an Additive Feature attribution method that satisfies all 3 properties are the Shapley values defined as:\\
\begin{align}\label{ShapleyEquation}
\phi_i(f, x) = \sum_{z' \subseteq x'} \frac{|z'|!(M - |z'| - 1)!}{M!}[f_x(z') - f_x(z' \setminus i)]
\end{align}
$z' \subseteq x'$ are all $z'$ vectors where the non-zero entries are a subset of the non-zero entries in $x'$.\\
$|z'|$ represents the number of non-zero entries in $z'$.\\

This means that at least in theory Additive Feature Attribution methods using Shapley Values as effects are to be preferred over other methods. 
In practice methods using shapely values can be intractable for larger sets.\\
In the next section we will introduce the SHAP framework which tries to unify these measures of feature importance.
%TODO check these statements after writing the next section.

\section{SHAP values}
%TODO mention global interpretations
SHAP (SHapley Additive exPlanations) values \cite{b2} are obtained by solving equation \ref{ShapleyEquation} using a conditional expectation function of the original model:
\begin{align}
f(h_x(z')) = E[f(z) | z_S]
\end{align}
S is the set of non-zero indexes in $z'$. 
The mapping function is therefore implicitly defined as:\\
\begin{align}
h_x(z') = z_S
\end{align}
$z_S$ has missing values for features not in the set S.\\
One problem with this approach is that in general models don't can't necessarily handle features that are just missing.\\
To address this issue $f(z_S)$ can be approximated with $E[f(z)| z_S].$\\
The intuition being that SHAP values  calculate the attribution (or effect) of each feature using the change in expectation of the model prediction when you condition on that feature. 
The feature $\phi_0 = E[f(z)]$ is the base value or the prediction the model would make if it had no observation. 
The following $\phi_i$ are then calculated by conditioning on some input features and averaging over all possible orderings of them.
As hinted already earlier the exact computation of SHAP values can be challenging as it requires to calculate combinatory many expectations. 
Assuming feature independence (equation \ref{independence}) and model linearity (equation \ref{linearity}) can make the computation much easier. 
\begin{align}
f(h_x(z')) &= E[f(z)|z_{S}]\\
&= E_{z_{\overline{S}}|z_S}[f(z)]\\
&\approx E_{z_{\overline{S}}}[f(z)] \label{independence}\\ 
&\approx f([z_S, E[z_{\overline{S}}]])  \label{linearity}
\end{align}
It allows you to 
These are however strong assumptions as most modern models are non-linear and features are in general not independent of each other (e.g. a pixel value only has a meaningful interpretation when put into the context of the surrounding pixels).\\
In the following subsections we will discuss algorithms for computing SHAP values.

\subsection{Kernel SHAP (Linear Lime + Shapely Value)}
Kernel SHAP \cite{b2} is the only method presented here that can be applied to any model.
Linear Lime is a method that minimizes the following objective function:
\begin{align}
\xi = \argmin_{g \in G} L(f,g,\pi_{x'}) + \Omega(g) \label{Lime}
\end{align}
Lime is a local linear explanation model that locally approximates the model around a given prediction.
Kernel SHAP now chooses $\Omega, \pi$ and $L$ in a way that recovers the Shapely Values (and therefore guarantees the properties missingness, local accuracy and consistency). This is possible because LIME is a additive feature attribution method. This lead to the following equations:
\begin{align}
\Omega(g) &= 0\\
\pi_{x'}(z') &= \frac{(M - 1)}{(M choose |z'|)|z'|(M - |z'|)1}\\
L(f,g,\pi_{x'}) &= \sum_{z' \in Z}[f(h_x(z')) - g(z')]^2 \pi_{x'}(z')
\end{align}
Because the loss is quadratic and $g(z')$ is assumed to be linear we can solve equation \ref{Lime} using linear regression (and calculate the shapely values using weighted linear regression). 
The input mapping used by LIME is equivalent to equation \ref{linearity} and we can therefore use Kernel SHAP on every model.
The simplified input mapping $h_x(z')$ that LIME uses is equivalent to equation \ref{linearity} which leads to estimation of the SHAP values.\\
To summarize Kernel SHAP is a model agnostic algorithm that under the (strong) assumptions model linearity and feature independence approximates the SHAP values. The time complexity of the algorithm is exponential in the number of input features M : $O(M2^M)$\cite{b2}.\\
An additional problem that arises in practice is that most models can not handle arbitrary patterns of missing inputs at inference time (which is a requirement to compute the Shapley values).
Implementation of the algorithm therefore make further approximations by replacing missing features with random values from a background dataset provided by the user (e.g. the training set). 
Given these severe flaws one can question whether the explanations provided by Kernel SHAP can be trusted in a realistic setting.\\
Better algorithms for calculating SHAP values for specific model types exist. Some of which will be covered in the following subsections:\\

\subsection{Linear SHAP}
For a linear model $f(x) = \sum_{j=1}^M w_j x_j + b$ you can approximate the SHAP values by only assuming feature independence (equation \ref{independence}).
The features will then be:
\begin{align}
\phi_0(f,x) = b
\phi_i(f,x) = w_j(x_j - E[x_j]) 
\end{align}
Notice that the difference to just looking at the weights is that Linear SHAP takes the difference to the baseline into account.

\subsection{Deep SHAP}
Deep SHAP \cite{b2} is an algorithm that combines shapely values with DeepLIFT \cite{b5}.
It combines SHAP values that are analytically computed for smaller components of the network and combines them into SHAP values for the whole network. Doing so we are linearising the non linear components of the network and the way in which we do this is uniquely specified by the Shapley values.
To calculate the expectation needed for SHAP we again have to assume that the input features are independent and that the deep models is linear. 
Notice that the second assumption is always violated as the point of adding depth to a neural network is that it becomes non-linear.
Further details on the algorithm are omitted here an can be found in \cite{b2}.

\subsection{Tree SHAP}
The previously discussed algorithms were all approximations of the SHAP values. The Tree SHAP \cite{b1} algorithm is able to compute \textbf{exact} SHAP values in \textbf{polynomial} time complexity: $O(TLD^2)$. 
T here is the number of trees in the ensemble, L is the maximum number of leaves in any of the trees and D is the maximum depth of any of these trees. 
This means that the algorithm is tractable even for high dimensional data (which Kernel Shap due to the exponential time complexity is not).\\
Global feature importance of trees can also be computed with methods based on Gain \cite{b9}, Split Count \cite{b10} or Permutations \cite{b11} and locally using Sabbas \cite{b12}.
However these methods are inconsistent, meaning features can become more important while the attribution decreases. One can relate this intuitively to the definition of consistency above (but the methods here are not necessarily additive feature attribution methods hence the lack of formality).\\
Tree SHAP does not have this problem as it uses (exact) Shapely values and therefore guarantees the properties local accuracy, missingness and consistency. 
The intuition of the algorithm is that it is recursively tracking the proportion of all possible subsets flowing down into the leaves of the tree. 
Doing so the algorithm requires polynomial memory in the order $O(D^2 + M)$.
The algorithm is applicable to trees and sums of trees (which uses the fact that SHAP values of two functions added together is the sum of their individual SHAP values).\\
Note that Tree SHAP does \textbf{not} require the additional assumptions model linearity and feature independence that we had to make earlier. The algorithmic description in pseudo code can be found in \cite{b1} and an open source implementation is available at \cite{b13}.\\

Now that we have a tool to precisely calculate SHAP values we can both globally for an entire dataset and locally on an individual sample inspect the importance of each feature as learned by the tree model gain insight into the model behaviour and potentially even the underlying problem if the model is good.
This analysis can even be further expanded be calculating SHAP interaction values covered next.

\subsection{SHAP interaction values}
Using the more modern Shapely interaction index \cite{b14} one can calculate interaction effects between features and store them in a matrix \cite{b1}. It's entries represent the impact of pairs of features on the model behaviour. The Shapely Interaction Index is defined as follows:\\
\begin{align}
\phi_{i,j} &= \sum_{S \subseteq N \setminus \{i, j\}} \frac{|S|!(M - |S| - 2)!}{2(M - 1)!}\nabla_{ij}(S), i\neq j \\
\nabla_{ij}(S) &= f_X(S \cup \{i,j\}) - f_X(S \cup \{i\}) - f_X(S \cup \{j\}) + f_X(S)
\end{align}

Due to it's similarity with the classical Shapely values the Shapely interaction values can be computed by using the Tree Shap algorithm twice for each feature leading to a runtime of $O(TMLD^2)$. These SHAP interaction values now allow us to distinguish between the main effect of a feature and it's interaction effect with other features. The main effect being defined as:
\begin{align}
\phi_{i,i} = \phi_i - \sum_{j \neq i} \phi_{i,j}
\end{align}


\subsection{Supervised Clustering}
Another interesting application of the individual feature attributions obtained by SHAP is called Supervised Clustering \cite{b1}. The idea is relatively straight forward. Instead of clustering features directly in an unsupervised fashion you first train an algorithm on the data using labels. You then calculate the corresponding SHAP values and then cluster on the feature attributions. With traditional clustering you have the problem that differences in the units of the features can have a large impact. With supervised clustering you don't have that problem because all attributions have the same unit as the models output. 
Fluctuations in the features value are also only considered if they are relevant for our (supervised) model.

\subsection{SHAP plots}
%TODO after you added some plots to the paper cite them here as examples.
Now there are multiple ways to visualize the explanations generated by the SHAP framework \cite{b1}.
The most traditional would be a bar chart where the global importance of each feature is plotted as absolute value. To plot the effects of a single example one can plot for example on a line how much each feature pushed the prediction of the model away from the baseline.\\
Additionally you can create \textit{SHAP summary plots} which add more detail to the global view. In these features are again sorted by importance each individual attribution is then plotted as a dot plotted horizontally where the x axis show the SHAP value (effect). If multiple dots have the same SHAP value they get stacked creating an effect similar to violin plots. The actual feature value is then encoded through color.\\
\textit{SHAP dependence plots} are another visual tool where the SHAP value of a feature is plotted axis against it's feature value on the x-axis. The individual points of the test set are then displayed like a scatter plot. An additional value of a different feature can be encoded in the color of the dots.


\section{Decision and Hoeffding Trees}










\subsection{Abbreviations and Acronyms}\label{AA}
Define abbreviations and acronyms the first time they are used in the text, 
even after they have been defined in the abstract. Abbreviations such as 
IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
abbreviations in the title or heads unless they are unavoidable.

\subsection{Equations}
Number equations consecutively. To make your 
equations more compact, you may use the solidus (~/~), the exp function, or 
appropriate exponents. Italicize Roman symbols for quantities and variables, 
but not Greek symbols. Use a long dash rather than a hyphen for a minus 
sign. Punctuate equations with commas or periods when they are part of a 
sentence, as in:
\begin{equation}
a+b=\gamma\label{eq}
\end{equation}

\subsection{\LaTeX-Specific Advice}

Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
of ``hard'' references (e.g., \verb|(1)|). That will make it possible
to combine sections, add equations, or change the order of figures or
citations without having to go through the file line by line.

\subsection{Figures and Tables}
\paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
bottom of columns. Avoid placing them in the middle of columns. Large 
figures and tables may span across both columns. Figure captions should be 
below the figures; table heads should appear above the tables. Insert 
figures and tables after they are cited in the text. Use the abbreviation 
``Fig.~\ref{fig}'', even at the beginning of a sentence.

\begin{table}[htbp]
\caption{Table Type Styles}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
\cline{2-4} 
\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
\hline
copy& More table copy$^{\mathrm{a}}$& &  \\
\hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\begin{figure}[htbp]
\centerline{\includegraphics{fig1.png}}
\caption{Example of a figure caption.}
\label{fig}
\end{figure}

\section{Methodology}

\section{Evaluation}

\section{Conclusion}

\section{Reproducibility Considerations}

\subsection{Software Libraries:}
The software library versions we used (read out using pip --freeze) are documented in table \ref{softwareVersion}.\\
All experiments were conducted on a single personal computer. The hardware specs can be found in table \ref{hardware}.\\


%TODO this are too many remove the uneccesary once.
\begin{table}[htbp]
\caption{Software Library Versions}
\begin{center}
\begin{tabular}{|c|c|}
	\hline
	\textbf{Software library} & \textbf{Version} \\
	\hline
	Python & 3.7.4\\ \hline
	anaconda-client & 1.7.2 \\ \hline
	anaconda-navigator & 1.9.7 \\ \hline
	anaconda-project & 0.8.3\\ \hline
	conda & 4.7.12\\ \hline
	conda-build & 3.18.9\\ \hline
	conda-package-handling & 1.6.\\ \hline0
	conda-verify & 3.4.2\\ \hline
	ipykernel & 5.1.2\\ \hline
	ipython & 7.8.0\\ \hline
	ipython-genutils & 0.2.0\\ \hline
	jupyter & 1.0.0\\ \hline
	jupyter-client & 5.3.3\\ \hline
	jupyter-console & 6.0.0\\ \hline
	jupyter-core & 4.5.0\\ \hline
	jupyterlab & 1.1.4\\ \hline
	jupyterlab-server & 1.0.6\\ \hline
	matplotlib & 3.1.1\\ \hline
	notebook & 6.0.1\\ \hline
	numpy & 1.16.5\\ \hline
	pandas & 0.25.1\\ \hline
	scikit-image & 0.15.0\\ \hline
	scikit-learn & 0.21.3\\ \hline
	scikit-multiflow & 0.5.0\\ \hline                 
	scipy & 1.3.1\\ \hline
	shap & 0.35.0\\ \hline
	sklearn & 0.0\\ \hline
	torch & 1.5.0+cu101\\ \hline
	torchvision & 0.6.0+cu101  \\ \hline
	
\end{tabular}
\label{softwareVersion}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{Hardware Specifications:}
\begin{center}
\begin{tabular}{|c|c|}
	\hline
	\textbf{Component} & \textbf{Model} \\
	\hline
	Processor & Intel Core i7-6700 CPU\\ \hline
	RAM & 16 GB Corsair Vengeance LPX DDR4-2400\\ \hline
	GPU & NVIDIA GeForce GTX 980 \\ \hline
	Motherboard & ASUS Z170 Pro Gaming \\ \hline
	Operating system & Microsoft Windows 10 Education N \\ \hline
	Hard Drive & SSD Crucial MX200 \\ \hline
\end{tabular}
\label{hardware}
\end{center}
\end{table}





\section*{Acknowledgment}

The preferred spelling of the word ``acknowledgment'' in America is without 
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page.

\begin{thebibliography}{00}
\bibitem{b1} Scott M. Lundberg, Gabriel G. Erion, Su-In Lee:
Consistent Individualized Feature Attribution for Tree Ensembles. CoRR abs/1802.03888 (2018).
\bibitem{b2} Scott M. Lundberg, Su-In Lee: A Unified Approach to Interpreting Model Predictions. NIPS 2017: 4765-4774.
\bibitem{b3} Pedro M. Domingos, Geoff Hulten: Mining high-speed data streams. KDD 2000: 71-80
\bibitem{b4} Marco Túlio Ribeiro, Sameer Singh, Carlos Guestrin: "Why Should I Trust You?": Explaining the Predictions of Any Classifier. KDD 2016: 1135-1144.
\bibitem{b5} Avanti Shrikumar, Peyton Greenside, Anshul Kundaje: Learning Important Features Through Propagating Activation Differences. ICML 2017: 3145-3153.
\bibitem{b6} Alexander Binder, Grégoire Montavon, Sebastian Bach, Klaus-Robert Müller, Wojciech Samek: Layer-wise Relevance Propagation for Neural Networks with Local Renormalization Layers. CoRR abs/1604.00825 (2016).
\bibitem{b7}Anupam Datta, Shayak Sen, Yair Zick: Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems. IEEE Symposium on Security and Privacy 2016: 598-617.
\bibitem{b8}Erik Strumbelj, Igor Kononenko: Explaining prediction models and individual predictions with feature contributions. Knowl. Inf. Syst. 41(3): 647-665 (2014).
\bibitem{b9} Leo Breiman, J. H. Friedman, R. A. Olshen, C. J. Stone: Classification and Regression Trees. Wadsworth 1984, ISBN 0-534-98053-8.
\bibitem{b10}Tianqi Chen, Carlos Guestrin: XGBoost: A Scalable Tree Boosting System. KDD 2016: 785-794.
\bibitem{b11}L Auret, C Aldrich: Empirical comparison of tree ensemble variable importance measures. Chemometrics and Intelligent Laboratory Systems, 2011 - 105, 2(2011), 157-170
\bibitem{b12}Ando Sabbas. 2014. Interpreting random forests. http://blog.datadive.net/interpreting-random-forests/ Accessed: 2020-06-15
\bibitem{b13}https://github.com/slundberg/shap Accessed: 2020-06-15
\bibitem{b14}Katsushige Fujimoto, Ivan Kojadinovic, Jean-Luc Marichal: Axiomatic characterizations of probabilistic and cardinal-probabilistic interaction indices. Games Econ. Behav. 55(1): 72-99 (2006).

\end{thebibliography}

\end{document}
