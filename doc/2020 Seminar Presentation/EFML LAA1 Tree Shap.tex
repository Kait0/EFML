\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}

\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\DeclareMathOperator*{\argmin}{\arg\!\min}

    
\begin{document}

\title{Explaining Tree Models}

\author{
	\IEEEauthorblockN{Bernhard Jaeger}
	\IEEEauthorblockA{
		\textit{University of Tübingen}\\
		Tübingen, Germany \\
		bernhard.jaeger@student.uni-tuebingen.de
	}
}

\maketitle

\begin{abstract}
Understanding the decisions made by machine learning models can be challenging due to their black box nature.
We investigate the additive feature attribution method SHAP (SHapley Additive exPlanations) that addresses this problem \cite{b2}. 
While SHAP can be used to explain the decisions of any model, the generic KernelSHAP algorithm for it can be intractable for larger datasets. 
For tree models there exists a specialized algorithm (Tree SHAP) that can compute exact SHAP values in polynomial time \cite{b1}. 
We use the SHAP framework to investigate the relationship between Decision Trees and Hoeffding Trees\cite{b3}. 
Theory tells us that a Hoeffding Tree should converge towards a Decision Tree when trained. 
In our experiments however we could not observe convergence. 
The reason for this remains unclear.
\end{abstract}

\begin{IEEEkeywords}
Tree SHAP, SHAP, Decision Tree, Hoeffding Tree
\end{IEEEkeywords}

\section{Introduction}
This . This  and \cite{b3} document is a model and instructions for \LaTeX.
Please observe the conference page limits. 

\section{Additive Feature Attribution methods}
Modern models of Machine Learning have become increasingly complex making it hard to explain their decisions.
To address this challenge we can go one layer of abstraction further and make a model of the model which we want to be simple enough to be interpretable again.
In \cite{b2} they coined the term explanation model for this approach of using an interpretable approximation of the original model.\\
When viewed trough this lens multiple of the current explanation methods can be categorized in the class of  \textbf{Additive Feature Attribution methods}. In \cite{b2} they defined this as methods having an explanation model that is a linear function of binary variables:\\

\begin{align}
	g(z') = \phi_0 + \sum_{i=1}^{M}\phi_i z_{i}'
\end{align}

Where g is the explanation model, $z' \in \{0,1\}^M, \phi \in \mathbb{R}$. \\
\textit{Local methods} like this first approximate their model $f(x)$ using \textit{simplified inputs} $x'$ where $x = h_x(x')$.\\
The objective is then to ensure $g(z') \approx f(h_x(z'))$ whenever $z' \approx x'$. \\
$\phi_i$ is then called the \textbf{effect} of feature i and summing up all effects approximates the output of the model $f(x)$ we wish to explain. Note that these \textit{explanation models} are now easily interpretable. We can for a prediction $f(x)$ given input $x$  say what influence or effect each component $x_i$ had on the final decision.\\
Methods falling in the category of Additive Feature Attribution methods include LIME \cite{b4}, DeepLIFT\cite{b5}, layer-wise relevance propagation \cite{b6}, Quantitive Input Influence \cite{b7}, Shapley sampling values \cite{b8} and SHAP \cite{b2}.\\

Additive Feature Attribution methods can have 3 desirable theoretical properties \cite{b2}:\\

\textbf{Local Accuracy}: \\
\begin{align}
f(x) = g(x') when x = h_x(x') and \phi_0 = f(h_x(0))
\end{align}
The intuition being that the explanation model should match the original model for the simplified input (notice $x' = z'$ here). \\

\textbf{Missingness}: \\
\begin{align}
x'_i = 0 \rightarrow \phi_i = 0
\end{align}
The intuition is that feature missing in the original input may have no impact even if they are present in the simplified version. (Notice that even if $x' = 0$, $z'$ does not necessarily have to be 0).\\

\textbf{Consistency}: \\
$f_x(z') := f(h_x(z'))$, and let $z' \setminus i$ be $z'_i = 0$\\
For any two models f and f', if:
\begin{align}
f'_x(z') - f'_x(z' \setminus i) \geq f_x(z') - f_x(z' \setminus i)
\end{align}
for all inputs $z' \in {0,1}^M$. \\
Meaning that the model changes in a way that some simplified input's contribution becomes larger.\\
Then: $ \phi_i(f', x) \geq \phi_i(f,x)$ \\
Meaning that it's inputs attribution should not become smaller.\\

Now it turns out that the only possible features for an Additive Feature attribution method that satisfies all 3 properties are the Shapley values defined as:\\
\begin{align}\label{ShapleyEquation}
\phi_i(f, x) = \sum_{z' \subseteq x'} \frac{|z'|!(M - |z'| - 1)!}{M!}[f_x(z') - f_x(z' \setminus i)]
\end{align}
$z' \subseteq x'$ are all $z'$ vectors where the non-zero entries are a subset of the non-zero entries in $x'$.\\
$|z'|$ represents the number of non-zero entries in $z'$.\\

This means that at least in theory Additive Feature Attribution methods using Shapley Values as effects are to be preferred over other methods. 
In practice methods using shapely values can be intractable for larger sets.\\
In the next section we will introduce the SHAP framework which tries to unify these measures of feature importance.
%TODO check these statements after writing the next section.

\section{SHAP values}
SHAP (SHapley Additive exPlanations) values \cite{b2} are obtained by solving equation \ref{ShapleyEquation} using a conditional expectation function of the original model:
\begin{align}
f(h_x(z')) = E[f(z) | z_S]
\end{align}
S is the set of non-zero indexes in $z'$. 
The mapping function is therefore implicitly defined as:\\
\begin{align}
h_x(z') = z_S
\end{align}
$z_S$ has missing values for features not in the set S.\\
One problem with this approach is that in general models don't can't necessarily handle features that are just missing.\\
To address this issue $f(z_S)$ can be approximated with $E[f(z)| z_S].$\\
The intuition being that SHAP values  calculate the attribution (or effect) of each feature using the change in expectation of the model prediction when you condition on that feature. 
The feature $\phi_0 = E[f(z)]$ is the base value or the prediction the model would make if it had no observation. 
The following $\phi_i$ are then calculated by conditioning on some input features and averaging over all possible orderings of them.
As hinted already earlier the exact computation of SHAP values can be challenging as it requires to calculate combinatory many expectations. 
Assuming feature independence (equation \ref{independence}) and model linearity (equation \ref{linearity}) can make the computation much easier. 
\begin{align}
f(h_x(z')) &= E[f(z)|z_{S}]\\
&= E_{z_{\overline{S}}|z_S}[f(z)]\\
&\approx E_{z_{\overline{S}}}[f(z)] \label{independence}\\ 
&\approx f([z_S, E[z_{\overline{S}}]])  \label{linearity}
\end{align}
It allows you to 
These are however strong assumptions as most modern models are non-linear and features are in general not independent of each other (e.g. a pixel value only has a meaningful interpretation when put into the context of the surrounding pixels).\\
In the following subsections we will discuss algorithms for computing SHAP values.

\subsection{Kernel SHAP (Linear Lime + Shapely Value)}
Kernel SHAP \cite{b2} is the only method presented here that can be applied to any model.
Linear Lime is a method that minimizes the following objective function:
\begin{align}
\xi = \argmin_{g \in G} L(f,g,\pi_{x'}) + \Omega(g) \label{Lime}
\end{align}
Lime is a local linear explanation model that locally approximates the model around a given prediction.
Kernel SHAP now chooses $\Omega, \pi$ and $L$ in a way that recovers the Shapely Values (and therefore guarantees the properties missingness, local accuracy and consistency). This is possible because LIME is a additive feature attribution method. This lead to the following equations:
\begin{align}
\Omega(g) &= 0\\
\pi_{x'}(z') &= \frac{(M - 1)}{(M choose |z'|)|z'|(M - |z'|)1}\\
L(f,g,\pi_{x'}) &= \sum_{z' \in Z}[f(h_x(z')) - g(z')]^2 \pi_{x'}(z')
\end{align}
Because the loss is quadratic and $g(z')$ is assumed to be linear we can solve equation \ref{Lime} using linear regression (and calculate the shapely values using weighted linear regression). 
The input mapping used by LIME is equivalent to equation \ref{linearity} and we can therefore use Kernel SHAP on every model.
The simplified input mapping $h_x(z')$ that LIME uses is equivalent to equation \ref{linearity} which leads to estimation of the SHAP values.\\
To summarize Kernel SHAP is a model agnostic algorithm that under the (strong) assumptions model linearity and feature independence approximates the SHAP values. The time complexity of the algorithm is exponential in the number of input features M. In the original paper the time complexity was stated as $O(2^M + M^3)$ \cite{b2}. The ALIBI implementation of the algorithm claims $O(M2^M)$ as time complexity. \cite{b9}\\
An additional problem that arises in practice is that most models can not handle arbitrary patterns of missing inputs at inference time (which is a requirement to compute the Shapley values).
Implementation of the algorithm therefore make further approximations by replacing missing features with random values from a background dataset provided by the user (e.g. the training set). 
Given these severe flaws one can question whether the explanations provided by Kernel SHAP can be trusted in a realistic setting.\\
Better algorithms for calculating SHAP values for specific model types exist and will be covered in the following subsections:\\

\subsection{Linear SHAP}

\section{Decision and Hoeffding Trees}

\subsection{Abbreviations and Acronyms}\label{AA}
Define abbreviations and acronyms the first time they are used in the text, 
even after they have been defined in the abstract. Abbreviations such as 
IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
abbreviations in the title or heads unless they are unavoidable.

\subsection{Equations}
Number equations consecutively. To make your 
equations more compact, you may use the solidus (~/~), the exp function, or 
appropriate exponents. Italicize Roman symbols for quantities and variables, 
but not Greek symbols. Use a long dash rather than a hyphen for a minus 
sign. Punctuate equations with commas or periods when they are part of a 
sentence, as in:
\begin{equation}
a+b=\gamma\label{eq}
\end{equation}

\subsection{\LaTeX-Specific Advice}

Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
of ``hard'' references (e.g., \verb|(1)|). That will make it possible
to combine sections, add equations, or change the order of figures or
citations without having to go through the file line by line.

\subsection{Figures and Tables}
\paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
bottom of columns. Avoid placing them in the middle of columns. Large 
figures and tables may span across both columns. Figure captions should be 
below the figures; table heads should appear above the tables. Insert 
figures and tables after they are cited in the text. Use the abbreviation 
``Fig.~\ref{fig}'', even at the beginning of a sentence.

\begin{table}[htbp]
\caption{Table Type Styles}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
\cline{2-4} 
\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
\hline
copy& More table copy$^{\mathrm{a}}$& &  \\
\hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\begin{figure}[htbp]
\centerline{\includegraphics{fig1.png}}
\caption{Example of a figure caption.}
\label{fig}
\end{figure}

\section{Methodology}

\section{Evaluation}

\section{Conclusion}

\section{Reproducibility Considerations}

\subsection{Software Libraries:}
The software library versions we used (read out using pip --freeze) are documented in table \ref{softwareVersion}.\\
All experiments were conducted on a single personal computer. The hardware specs can be found in table \ref{hardware}.\\


%TODO this are too many remove the uneccesary once.
\begin{table}[htbp]
\caption{Software Library Versions}
\begin{center}
\begin{tabular}{|c|c|}
	\hline
	\textbf{Software library} & \textbf{Version} \\
	\hline
	Python & 3.7.4\\ \hline
	anaconda-client & 1.7.2 \\ \hline
	anaconda-navigator & 1.9.7 \\ \hline
	anaconda-project & 0.8.3\\ \hline
	conda & 4.7.12\\ \hline
	conda-build & 3.18.9\\ \hline
	conda-package-handling & 1.6.\\ \hline0
	conda-verify & 3.4.2\\ \hline
	ipykernel & 5.1.2\\ \hline
	ipython & 7.8.0\\ \hline
	ipython-genutils & 0.2.0\\ \hline
	jupyter & 1.0.0\\ \hline
	jupyter-client & 5.3.3\\ \hline
	jupyter-console & 6.0.0\\ \hline
	jupyter-core & 4.5.0\\ \hline
	jupyterlab & 1.1.4\\ \hline
	jupyterlab-server & 1.0.6\\ \hline
	matplotlib & 3.1.1\\ \hline
	notebook & 6.0.1\\ \hline
	numpy & 1.16.5\\ \hline
	pandas & 0.25.1\\ \hline
	scikit-image & 0.15.0\\ \hline
	scikit-learn & 0.21.3\\ \hline
	scikit-multiflow & 0.5.0\\ \hline                 
	scipy & 1.3.1\\ \hline
	shap & 0.35.0\\ \hline
	sklearn & 0.0\\ \hline
	torch & 1.5.0+cu101\\ \hline
	torchvision & 0.6.0+cu101  \\ \hline
	
\end{tabular}
\label{softwareVersion}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{Hardware Specifications:}
\begin{center}
\begin{tabular}{|c|c|}
	\hline
	\textbf{Component} & \textbf{Model} \\
	\hline
	Processor & Intel Core i7-6700 CPU\\ \hline
	RAM & 16 GB Corsair Vengeance LPX DDR4-2400\\ \hline
	GPU & NVIDIA GeForce GTX 980 \\ \hline
	Motherboard & ASUS Z170 Pro Gaming \\ \hline
	Operating system & Microsoft Windows 10 Education N \\ \hline
	Hard Drive & SSD Crucial MX200 \\ \hline
\end{tabular}
\label{hardware}
\end{center}
\end{table}





\section*{Acknowledgment}

The preferred spelling of the word ``acknowledgment'' in America is without 
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page.

\begin{thebibliography}{00}
\bibitem{b1} Scott M. Lundberg, Gabriel G. Erion, Su-In Lee:
Consistent Individualized Feature Attribution for Tree Ensembles. CoRR abs/1802.03888 (2018).
\bibitem{b2} Scott M. Lundberg, Su-In Lee: A Unified Approach to Interpreting Model Predictions. NIPS 2017: 4765-4774.
\bibitem{b3} Pedro M. Domingos, Geoff Hulten: Mining high-speed data streams. KDD 2000: 71-80
\bibitem{b4} Marco Túlio Ribeiro, Sameer Singh, Carlos Guestrin: "Why Should I Trust You?": Explaining the Predictions of Any Classifier. KDD 2016: 1135-1144.
\bibitem{b5} Avanti Shrikumar, Peyton Greenside, Anshul Kundaje: Learning Important Features Through Propagating Activation Differences. ICML 2017: 3145-3153.
\bibitem{b6} Alexander Binder, Grégoire Montavon, Sebastian Bach, Klaus-Robert Müller, Wojciech Samek: Layer-wise Relevance Propagation for Neural Networks with Local Renormalization Layers. CoRR abs/1604.00825 (2016).
\bibitem{b7}Anupam Datta, Shayak Sen, Yair Zick: Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems. IEEE Symposium on Security and Privacy 2016: 598-617.
\bibitem{b8}Erik Strumbelj, Igor Kononenko: Explaining prediction models and individual predictions with feature contributions. Knowl. Inf. Syst. 41(3): 647-665 (2014).
\bibitem{b9} https://docs.seldon.io/projects/alibi/en/stable/methods/KernelSHAP.html .

\end{thebibliography}

\end{document}
